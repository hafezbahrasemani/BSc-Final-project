{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.data.ops.dataset_ops import CacheDataset\n",
    "from tensorflow_datasets.core import DatasetInfo\n",
    "print(\"tensorflow version: \", tf.__version__)\n",
    "\n",
    "# PASS GAN main parameters\n",
    "DATASET_NAME=\"rock_you\"\n",
    "NOISE_INPUT_SIZE = 128\n",
    "BACH_SIZE = 64\n",
    "EPOCHS = 199000\n",
    "LAYER_DIM = 128\n",
    "GRADIANT_PENALTY = 10\n",
    "OUTPUT_SEQ_LENGTH = 10\n",
    "\n",
    "# Adam Optimizer\"s hyper-parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Networks models based on the architecture in the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a class Residual block based on Residual Networks definition\n",
    "class ResidualBlock(tf.keras.Model):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.res_block = tf.keras.Sequential([\n",
    "            tf.keras.layers.ReLU(True),\n",
    "            tf.keras.layers.Conv1D(dim, dim, 5, padding='same'),\n",
    "            tf.keras.layers.ReLU(True),\n",
    "            tf.keras.layers.Conv1D(dim, dim, 5, padding='same'),\n",
    "        ])\n",
    "\n",
    "    def call(self, input_data, **kwargs):\n",
    "        output = self.res_block(input_data)\n",
    "        return input_data + (0.3 * output)\n",
    "\n",
    "# create a class Generator Network\n",
    "class GeneratorNetwork(tf.keras.Model):\n",
    "    def __init__(self, dim, pass_length):\n",
    "        self.dim = dim\n",
    "        self.pass_length = pass_length\n",
    "\n",
    "        # instantiate a Sequential Model\n",
    "        self.generator_res_block_model = tf.keras.models.Sequential()\n",
    "\n",
    "        # first linear layer\n",
    "        self.first_linear_layer = tf.keras.layers.Dense(128, activation='linear',\n",
    "                                                        input_shape=(dim*pass_length, ))\n",
    "\n",
    "        # residual blocks in a sequential order\n",
    "        self.generator_res_block_model.add(ResidualBlock(dim=5))\n",
    "        self.generator_res_block_model.add(ResidualBlock(dim=5))\n",
    "        self.generator_res_block_model.add(ResidualBlock(dim=5))\n",
    "        self.generator_res_block_model.add(ResidualBlock(dim=5))\n",
    "        self.generator_res_block_model.add(ResidualBlock(dim=5))\n",
    "\n",
    "        # convolutional 1D layer\n",
    "        self.conv_1d_layer = tf.keras.layers.Conv1D(64, 32, 1, padding='valid')\n",
    "\n",
    "        # last soft max layer\n",
    "        self.softmax_layer = tf.keras.layers.Softmax(axis=1)\n",
    "\n",
    "    def call(self, input_noise, **kwargs):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_noise: noise input of some sample generated passwords\n",
    "        :param kwargs:\n",
    "        :return: the generated passwords for an iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # feed first layer with noise data\n",
    "        output = self.first_linear_layer(input_noise)\n",
    "\n",
    "        # reshape the result of linear layer\n",
    "        output = tf.reshape(output, (-1, 2, 128))\n",
    "\n",
    "        # feed residual blocks by output from reshape stage\n",
    "        output = self.generator_res_block_model(output)\n",
    "        output = tf.reshape(output, (1, 32, 8))\n",
    "\n",
    "        # feed resulted data to convolutional layer\n",
    "        output = self.conv_1d_layer(output)\n",
    "\n",
    "        # transpose operation on the resulted output\n",
    "        output = tf.transpose(output)\n",
    "\n",
    "        # feed softmax layer with transposed output\n",
    "        output = self.softmax_layer(output)\n",
    "        output = tf.reshape(output, [2, 1, 32])\n",
    "\n",
    "        return output\n",
    "\n",
    "# create discriminator network\n",
    "class DiscriminatorNetwork(tf.keras.Model):\n",
    "    def __init__(self, dim, pass_length):\n",
    "        self.dim = dim\n",
    "        self.pass_length = pass_length\n",
    "\n",
    "        self.block = tf.keras.Sequential([\n",
    "            ResidualBlock(dim),\n",
    "            ResidualBlock(dim),\n",
    "            ResidualBlock(dim),\n",
    "            ResidualBlock(dim),\n",
    "            ResidualBlock(dim),\n",
    "        ])\n",
    "        self.conv1d = tf.keras.layers.Conv1D(dim, 32, 1, padding='valid')\n",
    "        self.linear = tf.keras.layers.Dense(pass_length * dim, activation='linear')\n",
    "\n",
    "    def call(self, input_data, **kwargs):\n",
    "        output = tf.transpose(input_data, [0, 2, 1])\n",
    "        output = self.conv1d(output)\n",
    "        output = self.block(output)\n",
    "        output = tf.reshape(output, (-1, 64, 4))\n",
    "        output = self.linear(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessingPipeLine:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _load_and_cache_dataset(self):\n",
    "        dataset = tfds.load(name=DATASET_NAME,\n",
    "                                      split='train[:75%]')\n",
    "        # print(\"[info] \", dataset_info)\n",
    "        dataset = dataset.shuffle(100000, reshuffle_each_iteration=True)\n",
    "\n",
    "        # Cache dataset for future use\n",
    "        dataset = dataset.cache()\n",
    "        return dataset\n",
    "\n",
    "    def choose_passwords_of_length_10_or_less(self, ds):\n",
    "        pass\n",
    "\n",
    "    def call(self):\n",
    "        initial_ds: CacheDataset = self._load_and_cache_dataset()\n",
    "        def sequence_length_filter(password: tf.Tensor):\n",
    "            print(password)\n",
    "            if len(password.get(\"password\").decode(\"utf-8\")) <= 10:\n",
    "                return password\n",
    "        ds = initial_ds.as_numpy_iterator()\n",
    "\n",
    "        ds = ds.filter(sequence_length_filter)\n",
    "        print(\"[info] dataset elements spec: \", initial_ds.element_spec)\n",
    "\n",
    "\n",
    "        # ds = initial_ds.filter()\n",
    "        # for data in initial_ds:\n",
    "        #     print(data)\n",
    "        # ds = list(filter(lambda x: len(x) <= 10, initial_ds))\n",
    "        # for password in ds:\n",
    "        #     print(password.get(\"password\").decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_NumpyIterator' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_195577/1523839463.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mPreprocessingPipeLine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_195577/428471853.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minitial_ds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_numpy_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m         \u001B[0mds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msequence_length_filter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"[info] dataset elements spec: \"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial_ds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0melement_spec\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_NumpyIterator' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "PreprocessingPipeLine().call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training(epochs=1000, batch_size=64, save_interval=500):\n",
    "    # Load the rock you dataset\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (BSc-Final-project)",
   "language": "python",
   "name": "pycharm-949d0fdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}